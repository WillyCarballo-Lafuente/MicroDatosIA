{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00dd439",
   "metadata": {},
   "source": [
    "# Comparación de PCA vs Autoencoder para Reducción de Dimensionalidad\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Este notebook compara dos técnicas de reducción de dimensionalidad aplicadas a datos de presión atmosférica (mean sea level pressure - MSL):\n",
    "\n",
    "1. **PCA (Principal Component Analysis)**: Método clásico de reducción de dimensionalidad basado en álgebra lineal\n",
    "2. **Autoencoder (CNN Autoencoder)**: Red neuronal profunda que aprende representaciones comprimidas de los datos\n",
    "\n",
    "## ¿Por qué comparar estas técnicas?\n",
    "\n",
    "- **PCA**: Método lineal, interpretable, rápido, pero limitado a relaciones lineales\n",
    "- **Autoencoder**: Método no lineal, puede capturar patrones complejos, pero requiere más recursos computacionales\n",
    "\n",
    "## Estructura del Notebook\n",
    "\n",
    "1. Carga y visualización de datos de presión atmosférica\n",
    "2. Aplicación de PCA para reducción de dimensionalidad\n",
    "3. Aplicación de Autoencoder CNN para reducción de dimensionalidad\n",
    "4. Comparación de resultados y análisis de correlación entre ambas técnicas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa2bcb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: No se puede encontrar el módulo especificado.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[31mImportError\u001b[39m: DLL load failed while importing _multiarray_umath: No se puede encontrar el módulo especificado."
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: No se puede encontrar el módulo especificado.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[31mImportError\u001b[39m: DLL load failed while importing _multiarray_umath: No se puede encontrar el módulo especificado."
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy._core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mCarga de datos de presión atmosférica (MSL - Mean Sea Level Pressure)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mde ERA5, un dataset de reanálisis climático.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m \u001b[33;03m- Período temporal: Desde el año 2000 en adelante\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxarray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxr\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Cargar datos de presión a nivel del mar desde servidor THREDDS\u001b[39;00m\n\u001b[32m     13\u001b[39m pressure_data = xr.open_dataset(\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhttps://geoocean.sci.unican.es/thredds/dodsC/geoocean/era5-msl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Willy\\miniforge3\\envs\\micro\\Lib\\site-packages\\xarray\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetadata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version \u001b[38;5;28;01mas\u001b[39;00m _version\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxarray\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m coders, groupers, indexes, testing, tutorial, ufuncs\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxarray\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     load_dataarray,\n\u001b[32m      6\u001b[39m     load_dataset,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     open_mfdataset,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxarray\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwriters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m save_mfdataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Willy\\miniforge3\\envs\\micro\\Lib\\site-packages\\xarray\\coders.py:6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mThis module provides coder objects that encapsulate the\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m\"encoding/decoding\" process.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxarray\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcoding\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtimes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CFDatetimeCoder, CFTimedeltaCoder\n\u001b[32m      8\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mCFDatetimeCoder\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCFTimedeltaCoder\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Willy\\miniforge3\\envs\\micro\\Lib\\site-packages\\xarray\\coding\\times.py:12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Union, cast\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OutOfBoundsDatetime, OutOfBoundsTimedelta\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxarray\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcoding\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     SerializationWarning,\n\u001b[32m     17\u001b[39m     VariableCoder,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     unpack_for_encoding,\n\u001b[32m     23\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Willy\\miniforge3\\envs\\micro\\Lib\\site-packages\\pandas\\__init__.py:49\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     50\u001b[39m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[32m     51\u001b[39m     ArrowDtype,\n\u001b[32m     52\u001b[39m     Int8Dtype,\n\u001b[32m     53\u001b[39m     Int16Dtype,\n\u001b[32m     54\u001b[39m     Int32Dtype,\n\u001b[32m     55\u001b[39m     Int64Dtype,\n\u001b[32m     56\u001b[39m     UInt8Dtype,\n\u001b[32m     57\u001b[39m     UInt16Dtype,\n\u001b[32m     58\u001b[39m     UInt32Dtype,\n\u001b[32m     59\u001b[39m     UInt64Dtype,\n\u001b[32m     60\u001b[39m     Float32Dtype,\n\u001b[32m     61\u001b[39m     Float64Dtype,\n\u001b[32m     62\u001b[39m     CategoricalDtype,\n\u001b[32m     63\u001b[39m     PeriodDtype,\n\u001b[32m     64\u001b[39m     IntervalDtype,\n\u001b[32m     65\u001b[39m     DatetimeTZDtype,\n\u001b[32m     66\u001b[39m     StringDtype,\n\u001b[32m     67\u001b[39m     BooleanDtype,\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[32m     69\u001b[39m     NA,\n\u001b[32m     70\u001b[39m     isna,\n\u001b[32m     71\u001b[39m     isnull,\n\u001b[32m     72\u001b[39m     notna,\n\u001b[32m     73\u001b[39m     notnull,\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[32m     75\u001b[39m     Index,\n\u001b[32m     76\u001b[39m     CategoricalIndex,\n\u001b[32m     77\u001b[39m     RangeIndex,\n\u001b[32m     78\u001b[39m     MultiIndex,\n\u001b[32m     79\u001b[39m     IntervalIndex,\n\u001b[32m     80\u001b[39m     TimedeltaIndex,\n\u001b[32m     81\u001b[39m     DatetimeIndex,\n\u001b[32m     82\u001b[39m     PeriodIndex,\n\u001b[32m     83\u001b[39m     IndexSlice,\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[32m     85\u001b[39m     NaT,\n\u001b[32m     86\u001b[39m     Period,\n\u001b[32m     87\u001b[39m     period_range,\n\u001b[32m     88\u001b[39m     Timedelta,\n\u001b[32m     89\u001b[39m     timedelta_range,\n\u001b[32m     90\u001b[39m     Timestamp,\n\u001b[32m     91\u001b[39m     date_range,\n\u001b[32m     92\u001b[39m     bdate_range,\n\u001b[32m     93\u001b[39m     Interval,\n\u001b[32m     94\u001b[39m     interval_range,\n\u001b[32m     95\u001b[39m     DateOffset,\n\u001b[32m     96\u001b[39m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[32m     97\u001b[39m     to_numeric,\n\u001b[32m     98\u001b[39m     to_datetime,\n\u001b[32m     99\u001b[39m     to_timedelta,\n\u001b[32m    100\u001b[39m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[32m    101\u001b[39m     Flags,\n\u001b[32m    102\u001b[39m     Grouper,\n\u001b[32m    103\u001b[39m     factorize,\n\u001b[32m    104\u001b[39m     unique,\n\u001b[32m    105\u001b[39m     value_counts,\n\u001b[32m    106\u001b[39m     NamedAgg,\n\u001b[32m    107\u001b[39m     array,\n\u001b[32m    108\u001b[39m     Categorical,\n\u001b[32m    109\u001b[39m     set_eng_float_format,\n\u001b[32m    110\u001b[39m     Series,\n\u001b[32m    111\u001b[39m     DataFrame,\n\u001b[32m    112\u001b[39m )\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtseries\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Willy\\miniforge3\\envs\\micro\\Lib\\site-packages\\pandas\\core\\api.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     NaT,\n\u001b[32m      3\u001b[39m     Period,\n\u001b[32m      4\u001b[39m     Timedelta,\n\u001b[32m      5\u001b[39m     Timestamp,\n\u001b[32m      6\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmissing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     ArrowDtype,\n\u001b[32m     11\u001b[39m     CategoricalDtype,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     PeriodDtype,\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Willy\\miniforge3\\envs\\micro\\Lib\\site-packages\\pandas\\_libs\\__init__.py:17\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Below imports needs to happen first to ensure pandas top level\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# module gets monkeypatched with the pandas_datetime_CAPI\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# see pandas_datetime_exec in pd_datetime.c\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minterval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtslibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     NaT,\n\u001b[32m     21\u001b[39m     NaTType,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     iNaT,\n\u001b[32m     27\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: numpy._core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Carga de datos de presión atmosférica (MSL - Mean Sea Level Pressure)\n",
    "de ERA5, un dataset de reanálisis climático.\n",
    "\n",
    "Los datos se cargan desde un servidor THREDDS y se seleccionan:\n",
    "- Región: Océano Pacífico (longitud 60-200°, latitud 20°N a 30°S)\n",
    "- Período temporal: Desde el año 2000 en adelante\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# Cargar datos de presión a nivel del mar desde servidor THREDDS\n",
    "pressure_data = xr.open_dataset(\n",
    "    \"https://geoocean.sci.unican.es/thredds/dodsC/geoocean/era5-msl\"\n",
    ")\n",
    "\n",
    "# Convertir el tiempo a formato datetime64 para operaciones temporales\n",
    "pressure_data[\"time\"] = np.char.decode(\n",
    "    pressure_data.time.values, encoding=\"utf-8\"\n",
    ").astype(\n",
    "    \"datetime64\"  # Convert to datetime64 for time operations\n",
    ")\n",
    "\n",
    "# Seleccionar región del Pacífico y período temporal, cargar en memoria\n",
    "pressure_data = pressure_data.sel(\n",
    "    longitude=slice(60, 200), latitude=slice(20, -30), time=slice(\"2000\", None)\n",
    ").load()\n",
    "\n",
    "# Mostrar información del dataset\n",
    "pressure_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fcfc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pressure_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77eccdae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: No se puede encontrar el módulo especificado.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[31mImportError\u001b[39m: DLL load failed while importing _multiarray_umath: No se puede encontrar el módulo especificado."
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy._core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mVisualización de los datos de presión atmosférica para una fecha específica.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mEsto nos permite ver la estructura espacial de los datos antes de aplicar\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mlas técnicas de reducción de dimensionalidad.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcartopy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcrs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mccrs\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Crear figura con proyección ortográfica centrada en el Pacífico\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Willy\\miniforge3\\envs\\micro\\Lib\\site-packages\\cartopy\\__init__.py:106\u001b[39m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Commonly used sub-modules. Imported here to provide end-user\u001b[39;00m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# convenience.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcartopy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcrs\u001b[39;00m  \u001b[38;5;66;03m# noqa: E402  module-level imports\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcartopy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature\u001b[39;00m  \u001b[38;5;66;03m# noqa: E402,F401  (unused import)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Willy\\miniforge3\\envs\\micro\\Lib\\site-packages\\cartopy\\crs.py:24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyproj\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Transformer\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyproj\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ProjError\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshapely\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshapely\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeometry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msgeom\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshapely\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprepared\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m prep\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Willy\\miniforge3\\envs\\micro\\Lib\\site-packages\\shapely\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"Manipulation and analysis of geometric objects in the Cartesian plane.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshapely\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GEOSException\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshapely\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Geometry\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshapely\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m geos_version, geos_version_string\n",
      "\u001b[31mImportError\u001b[39m: numpy._core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Visualización de los datos de presión atmosférica para una fecha específica.\n",
    "Esto nos permite ver la estructura espacial de los datos antes de aplicar\n",
    "las técnicas de reducción de dimensionalidad.\n",
    "\"\"\"\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear figura con proyección ortográfica centrada en el Pacífico\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": ccrs.Orthographic(130, -5)})\n",
    "\n",
    "# Visualizar presión para una fecha específica (15 de agosto de 2023)\n",
    "pressure_data.sel(time=\"2023-08-15\").msl.plot(\n",
    "    ax=ax,\n",
    "    cmap=\"bwr\",  # Colormap blue-white-red para presión\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    vmin=101300 - 3000,  # Rango de presión en Pascales\n",
    "    vmax=101300 + 3000,\n",
    ")\n",
    "ax.coastlines()  # Añadir líneas de costa para referencia geográfica\n",
    "plt.title(\"Presión Atmosférica (MSL) - 15 de Agosto 2023\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c01e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "APLICACIÓN DE PCA (Principal Component Analysis)\n",
    "\n",
    "PCA es un método de reducción de dimensionalidad que:\n",
    "1. Encuentra las direcciones (componentes principales) de máxima varianza en los datos\n",
    "2. Proyecta los datos sobre estos componentes para obtener una representación de menor dimensión\n",
    "3. Permite reconstruir los datos originales usando solo los componentes principales más importantes\n",
    "\n",
    "En este caso:\n",
    "- n_components=0.9: Retenemos suficientes componentes para explicar el 90% de la varianza\n",
    "- vars_to_stack: Apilamos la variable 'msl' (presión)\n",
    "- coords_to_stack: Apilamos las coordenadas espaciales (latitud, longitud)\n",
    "- pca_dim_for_rows: Cada fila representa un tiempo diferente\n",
    "\"\"\"\n",
    "from bluemath_tk.datamining.pca import PCA\n",
    "\n",
    "# Crear objeto PCA que retiene el 90% de la varianza\n",
    "pca = PCA(n_components=0.9)\n",
    "\n",
    "# Aplicar PCA: transformar datos espaciales (lat, lon) en componentes principales temporales\n",
    "latent_pcs = pca.fit_transform(\n",
    "    data=pressure_data,\n",
    "    vars_to_stack=[\"msl\"],  # Variable a analizar\n",
    "    coords_to_stack=[\"latitude\", \"longitude\"],  # Coordenadas espaciales a apilar\n",
    "    pca_dim_for_rows=\"time\",  # Dimensión temporal como filas\n",
    ")\n",
    "\n",
    "# Reconstruir los datos originales usando solo los componentes principales\n",
    "# Esto nos permite comparar la calidad de la reconstrucción\n",
    "pressure_data_pca = pca.inverse_transform(latent_pcs)\n",
    "pressure_data_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3654c30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Verificar cuántos componentes principales fueron necesarios para explicar el 90% de la varianza.\n",
    "Esto nos da una idea de la complejidad de los datos y cuánta reducción de dimensionalidad\n",
    "logramos con PCA.\n",
    "\"\"\"\n",
    "# Número de componentes principales retenidos\n",
    "pca.pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e30b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Visualización de los primeros 3 EOFs (Empirical Orthogonal Functions).\n",
    "Los EOFs son los patrones espaciales que corresponden a los componentes principales.\n",
    "Estos patrones muestran las estructuras espaciales más importantes en los datos.\n",
    "\"\"\"\n",
    "# Visualizar los primeros 3 EOFs (patrones espaciales principales)\n",
    "pca.plot_eofs(vars_to_plot=[\"msl\"], num_eofs=3, map_center=(150, -15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e6d539",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Comparación visual: Datos originales vs Reconstrucción PCA\n",
    "\n",
    "Esta visualización muestra:\n",
    "- Izquierda: Datos originales de presión\n",
    "- Derecha: Reconstrucción usando solo los componentes principales (90% varianza)\n",
    "\n",
    "Si la reconstrucción es buena, ambas imágenes deberían verse muy similares.\n",
    "\"\"\"\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear figura con dos subplots lado a lado\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=2,\n",
    "    subplot_kw={\"projection\": ccrs.Orthographic(150, -15)},\n",
    "    figsize=(15, 5),\n",
    ")\n",
    "\n",
    "# Visualizar datos originales\n",
    "pressure_data.sel(time=\"2022-08-15\").msl.plot(\n",
    "    ax=axes[0],\n",
    "    cmap=\"bwr\",\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    vmin=101300 - 3000,\n",
    "    vmax=101300 + 3000,\n",
    ")\n",
    "axes[0].coastlines()\n",
    "axes[0].set_title(\"Datos Originales\")\n",
    "\n",
    "# Visualizar reconstrucción PCA\n",
    "pressure_data_pca.sel(time=\"2022-08-15\").msl.plot(\n",
    "    ax=axes[1],\n",
    "    cmap=\"bwr\",\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    vmin=101300 - 3000,\n",
    "    vmax=101300 + 3000,\n",
    ")\n",
    "axes[1].coastlines()\n",
    "axes[1].set_title(\"Reconstrucción PCA (90% varianza)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9269bfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PREPARACIÓN DE DATOS PARA EL AUTOENCODER\n",
    "\n",
    "El autoencoder CNN requiere los datos en un formato específico:\n",
    "- Forma: (batch, channels, height, width) = (time, channel, latitude, longitude)\n",
    "- Normalización: Los datos deben estar normalizados entre 0 y 1 para entrenar la red neuronal\n",
    "\n",
    "Esta normalización es importante porque:\n",
    "1. Las redes neuronales convergen mejor con datos normalizados\n",
    "2. Evita problemas de gradientes que desaparecen o explotan\n",
    "\"\"\"\n",
    "# Reorganizar datos para formato compatible con CNN: (time, channel, lat, lon)\n",
    "X = (\n",
    "    pressure_data[\"msl\"]\n",
    "    .expand_dims({\"channel\": [0]})  # Añadir dimensión de canal (como imágenes RGB)\n",
    "    .transpose(\"time\", \"channel\", \"latitude\", \"longitude\")  # Reordenar dimensiones\n",
    "    .values  # Convertir a numpy array\n",
    ")\n",
    "\n",
    "# Normalización min-max: escalar valores entre 0 y 1\n",
    "X_max = np.max(X)\n",
    "X_min = np.min(X)\n",
    "X = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "# Verificar forma de los datos: (número de tiempos, canales, latitud, longitud)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb52f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CREACIÓN DEL AUTOENCODER CNN\n",
    "\n",
    "Un autoencoder es una red neuronal que:\n",
    "1. Encoder: Comprime los datos a una representación latente de menor dimensión (k=15)\n",
    "2. Decoder: Reconstruye los datos originales desde la representación latente\n",
    "\n",
    "El autoencoder aprende a:\n",
    "- Capturar las características más importantes de los datos\n",
    "- Reconstruir los datos originales con la menor pérdida posible\n",
    "\n",
    "k=15: Dimensión del espacio latente (similar al número de componentes en PCA)\n",
    "\"\"\"\n",
    "from bluemath_tk.deeplearning.autoencoders import CNNAutoencoder\n",
    "\n",
    "# Crear autoencoder CNN con espacio latente de dimensión 15\n",
    "# Esto es comparable a usar 15 componentes principales en PCA\n",
    "ae = CNNAutoencoder(k=15)\n",
    "\n",
    "# Verificar dispositivo (CPU o GPU) que se usará para el entrenamiento\n",
    "ae.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7147e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ENTRENAMIENTO DEL AUTOENCODER\n",
    "\n",
    "El autoencoder se entrena para minimizar el error de reconstrucción.\n",
    "Durante el entrenamiento:\n",
    "- El encoder aprende a comprimir los datos eficientemente\n",
    "- El decoder aprende a reconstruir los datos desde la representación comprimida\n",
    "- El proceso se repite durante múltiples épocas hasta convergencia\n",
    "\n",
    "epochs=20: Número de pasadas completas sobre los datos\n",
    "batch_size=64: Número de muestras procesadas antes de actualizar los pesos\n",
    "\"\"\"\n",
    "# Entrenar el autoencoder\n",
    "history = ae.fit(X, epochs=20, batch_size=64)\n",
    "\n",
    "# El historial contiene información sobre la pérdida durante el entrenamiento\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa1bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OBTENER REPRESENTACIÓN LATENTE DEL AUTOENCODER\n",
    "\n",
    "La representación latente Z es equivalente a los componentes principales en PCA.\n",
    "Cada fila de Z contiene la representación comprimida de un tiempo específico.\n",
    "Esta representación captura las características más importantes de los datos.\n",
    "\"\"\"\n",
    "# Codificar los datos al espacio latente (equivalente a transformación PCA)\n",
    "Z = ae.encode(X)  # Get latent representations\n",
    "\n",
    "# Verificar forma: (número de tiempos, dimensión latente k=15)\n",
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f75af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RECONSTRUCCIÓN DE DATOS CON AUTOENCODER\n",
    "\n",
    "El autoencoder reconstruye los datos desde la representación latente.\n",
    "Luego desnormalizamos los datos para compararlos con los originales.\n",
    "\"\"\"\n",
    "# Predecir (reconstruir) los datos desde la representación latente\n",
    "X_recon_norm = ae.predict(X)\n",
    "\n",
    "# Desnormalizar: convertir de [0,1] de vuelta a escala original\n",
    "X_recon = X_recon_norm * (X_max - X_min) + X_min\n",
    "\n",
    "# Verificar forma de los datos reconstruidos\n",
    "X_recon.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c31572d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ERROR DE RECONSTRUCCIÓN DEL AUTOENCODER\n",
    "\n",
    "Calculamos el error cuadrático medio (MSE) entre los datos originales\n",
    "y la reconstrucción del autoencoder. Un valor bajo indica buena reconstrucción.\n",
    "\n",
    "Nota: Este error está en la escala normalizada [0,1].\n",
    "\"\"\"\n",
    "# Calcular error cuadrático medio (MSE) en escala normalizada\n",
    "mse_autoencoder = np.mean((X - X_recon_norm) ** 2)\n",
    "print(f\"Error cuadrático medio (MSE) del Autoencoder: {mse_autoencoder:.6f}\")\n",
    "mse_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be5045c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "COMBINAR TODAS LAS RECONSTRUCCIONES EN UN SOLO DATASET\n",
    "\n",
    "Creamos un dataset que contiene:\n",
    "- msl: Datos originales\n",
    "- msl_pca: Reconstrucción usando PCA\n",
    "- msl_cae: Reconstrucción usando Autoencoder CNN (Convolutional Autoencoder)\n",
    "\n",
    "Esto facilita la comparación entre métodos.\n",
    "\"\"\"\n",
    "# Crear dataset combinado con todas las versiones\n",
    "pressure_data_ALL = pressure_data.copy()\n",
    "\n",
    "# Añadir reconstrucción PCA\n",
    "pressure_data_ALL[\"msl_pca\"] = pressure_data_pca[\"msl\"]\n",
    "\n",
    "# Añadir reconstrucción Autoencoder (remodelar a formato original)\n",
    "pressure_data_ALL[\"msl_cae\"] = (\n",
    "    (\"time\", \"latitude\", \"longitude\"),\n",
    "    X_recon.reshape(9132, 51, 141),  # Remodelar a (tiempo, latitud, longitud)\n",
    ")\n",
    "\n",
    "pressure_data_ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec77eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "COMPARACIÓN DE ERRORES DE RECONSTRUCCIÓN ESPACIAL\n",
    "\n",
    "Visualizamos el error cuadrático máximo a través del tiempo para cada método.\n",
    "Esto muestra dónde espacialmente cada método tiene más dificultades para reconstruir.\n",
    "\n",
    "- Izquierda: Error máximo de PCA\n",
    "- Derecha: Error máximo de Autoencoder\n",
    "\n",
    "Áreas con mayor error indican regiones donde la reconstrucción es menos precisa.\n",
    "\"\"\"\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear figura con dos subplots\n",
    "fig, axes = plt.subplots(\n",
    "    1, 2, figsize=(10, 5), subplot_kw={\"projection\": ccrs.Orthographic(130, -5)}\n",
    ")\n",
    "\n",
    "# Error máximo de PCA a través del tiempo\n",
    "((pressure_data_ALL[\"msl\"] - pressure_data_ALL[\"msl_pca\"]) ** 2).max(dim=\"time\").plot(\n",
    "    ax=axes[0],\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    cmap=\"hot\",  # Colormap para visualizar errores\n",
    ")\n",
    "axes[0].coastlines()\n",
    "axes[0].set_title(\"Error Máximo PCA (a través del tiempo)\")\n",
    "\n",
    "# Error máximo de Autoencoder a través del tiempo\n",
    "((pressure_data_ALL[\"msl\"] - pressure_data_ALL[\"msl_cae\"]) ** 2).max(dim=\"time\").plot(\n",
    "    ax=axes[1],\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    cmap=\"hot\",\n",
    ")\n",
    "axes[1].coastlines()\n",
    "axes[1].set_title(\"Error Máximo Autoencoder (a través del tiempo)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7821a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ANÁLISIS DE CORRELACIÓN: PCA vs AUTOENCODER\n",
    "\n",
    "Este análisis compara las representaciones latentes de ambos métodos:\n",
    "- ¿Capturan información similar?\n",
    "- ¿Hay correspondencia entre componentes principales y dimensiones latentes?\n",
    "\n",
    "Una correlación alta indica que ambos métodos están capturando patrones similares,\n",
    "aunque pueden hacerlo de manera diferente (lineal vs no lineal).\n",
    "\n",
    "Interpretación:\n",
    "- Valores cercanos a ±1: Alta correlación (mismo patrón)\n",
    "- Valores cercanos a 0: Baja correlación (patrones diferentes)\n",
    "- El cuadrado negro en la diagonal resalta las correspondencias más directas\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Obtener componentes principales de PCA\n",
    "pca_components = pca.pcs.PCs.values  # Shape: (n_times, n_components)\n",
    "\n",
    "# Obtener representación latente del autoencoder\n",
    "Z_values = Z  # Shape: (n_times, k)\n",
    "\n",
    "# Calcular matriz de correlación\n",
    "# Primero estandarizamos ambas matrices (media=0, std=1) para calcular correlación\n",
    "pca_std = (pca_components - pca_components.mean(axis=0)) / pca_components.std(axis=0)\n",
    "Z_std = (Z_values - Z_values.mean(axis=0)) / Z_values.std(axis=0)\n",
    "\n",
    "# Calcular matriz de correlación: cada columna de PCA con cada columna de Z\n",
    "correlation_matrix = np.dot(pca_std.T, Z_std) / (pca_components.shape[0] - 1)\n",
    "\n",
    "# Obtener dimensiones para crear DataFrame\n",
    "n_pca_components = pca_components.shape[1]\n",
    "n_z_components = Z_values.shape[1]\n",
    "\n",
    "# Crear DataFrame para mejor visualización\n",
    "correlation_df = pd.DataFrame(\n",
    "    correlation_matrix,\n",
    "    index=[f\"PC{i + 1}\" for i in range(n_pca_components)],\n",
    "    columns=[f\"Z{i + 1}\" for i in range(n_z_components)],\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Matriz de Correlación entre Componentes PCA y Dimensiones Latentes del Autoencoder:\"\n",
    ")\n",
    "print(correlation_df)\n",
    "print(f\"\\nForma: {correlation_matrix.shape}\")\n",
    "print(f\"Forma componentes PCA: {pca_components.shape}\")\n",
    "print(f\"Forma Z (autoencoder): {Z_values.shape}\")\n",
    "\n",
    "# Visualizar matriz de correlación\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.heatmap(\n",
    "    correlation_df,\n",
    "    annot=True,  # Mostrar valores en cada celda\n",
    "    fmt=\".3f\",  # Formato de 3 decimales\n",
    "    cmap=\"coolwarm\",  # Colormap: azul (negativo), blanco (cero), rojo (positivo)\n",
    "    center=0,\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    cbar_kws={\"label\": \"Correlación\"},\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# Resaltar cuadrados diagonales\n",
    "# La diagonal corresponde a PCi con Zi (donde i es el mismo índice)\n",
    "min_dim = min(n_pca_components, n_z_components)\n",
    "for i in range(min_dim):\n",
    "    # Añadir borde negro alrededor de elementos diagonales\n",
    "    rect = plt.Rectangle(\n",
    "        (i, i),\n",
    "        1,\n",
    "        1,\n",
    "        fill=False,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=2.5,\n",
    "        clip_on=False,\n",
    "    )\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.title(\n",
    "    \"Matriz de Correlación: Componentes PCA vs Dimensiones Latentes del Autoencoder\"\n",
    ")\n",
    "plt.xlabel(\"Dimensiones Latentes del Autoencoder (Z)\")\n",
    "plt.ylabel(\"Componentes Principales (PC)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "micro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
